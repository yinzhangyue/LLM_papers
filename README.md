# Large Language Models Papers

## CoT
1. **Chain of Thought Prompting Elicits Reasoning in Large Language Models.** 

   *Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou*  [[pdf](https://arxiv.org/abs/2201.11903)] 2022.01
 
2. **Self-Consistency Improves Chain of Thought Reasoning in Language Models.**  

   *Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou*  [[pdf](https://arxiv.org/abs/2203.11171)] 2022.03
   
3. **STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning.** 

   *Eric Zelikman, Yuhuai Wu, Noah D. Goodman*  [[pdf](https://arxiv.org/abs/2203.14465)] 2022.03
 
4. **PaLM: Scaling Language Modeling with Pathways.** 

   *Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel*  [[pdf](https://arxiv.org/abs/2204.02311)] 2022.04   
   
5. **Can language models learn from explanations in context?.** 

   *Andrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, Felix Hill*  [[pdf](https://arxiv.org/abs/2204.02329)] 2022.04   
   
6. **Inferring Implicit Relations with Language Models.** 

   *Uri Katz, Mor Geva, Jonathan Berant*  [[pdf](https://arxiv.org/abs/2204.13778)] 2022.04   
 
7. **The Unreliability of Explanations in Few-Shot In-Context Learning.**
  
   *Xi Ye, Greg Durrett* [[pdf](https://arxiv.org/abs/2205.03401)] 2022.05

8. **Large Language Models are Zero-Shot Reasoners.**
  
   *Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa* [[pdf](https://arxiv.org/abs/2205.11916)] 2022.05

9. **Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.**
  
   *Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, Ed Chi* [[pdf](https://arxiv.org/abs/2205.10625)] 2022.05
   
10. **Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning.**

    *Antonia Creswell, Murray Shanahan, Irina Higgins* [[pdf](https://arxiv.org/abs/2205.09712)] 2022.05

11. **On the Advance of Making Language Models Better Reasoners.**

    *Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen* [[pdf](https://arxiv.org/abs/2206.02336)] 2022.06

12. **Emergent Abilities of Large Language Models.**

    *Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus* [[pdf](https://arxiv.org/abs/2206.07682)] 2022.06

13. **Minerva: Solving Quantitative Reasoning Problems with Language Models.**

    *Posted by Ethan Dyer and Guy Gur-Ari, Research Scientists, Google Research, Blueshift Team* [[blog](https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html)] 2022.06

14. **JiuZhang: A Chinese Pre-trained Language Model for Mathematical Problem Understanding.**

    *Wayne Xin Zhao, Kun Zhou, Zheng Gong, Beichen Zhang, Yuanhang Zhou, Jing Sha, Zhigang Chen, Shijin Wang, Cong Liu, Ji-Rong Wen* [[pdf](https://arxiv.org/abs/2206.06315)] 2022.06

15. **A Dataset and Benchmark for Automatically Answering and Generating Machine Learning Final Exams**

    *Sarah Zhang, Reece Shuttleworth, Derek Austin, Yann Hicke, Leonard Tang, Sathwik Karnik, Darnell Granberry, Iddo Drori* [[pdf](https://arxiv.org/abs/2206.05442)] 2022.06
    
16. **Rationale-Augmented Ensembles in Language Models.**

    *Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou* [[pdf](https://arxiv.org/abs/2207.00747)] 2022.07

17. **Language Model Cascades.**

    *David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, Charles Sutton* [[pdf](https://arxiv.org/abs/2207.10342)] 2022.07

18. **Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango.**

    *Aman Madaan, Amir Yazdanbakhsh* [[pdf](https://arxiv.org/abs/2209.07686)] 2022.09

19. **Compositional Semantic Parsing with Large Language Models.**

    *Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou* [[pdf](https://arxiv.org/abs/2209.15003)] 2022.09

20. **Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning.**

    *Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan* [[pdf](https://arxiv.org/abs/2209.14610)] 2022.09

21. **Language Models are Multilingual Chain-of-Thought Reasoners.**

    *Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei* [[pdf](https://arxiv.org/abs/2210.03057)] 2022.10

22. **Automatic Chain of Thought Prompting in Large Language Models.**

    *Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola* [[pdf](https://arxiv.org/abs/2210.03493)] 2022.10


23. **Binding Language Models in Symbolic Languages.**

    *Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu* [[pdf](https://arxiv.org/abs/2210.02875)] 2022.10
    
24. **ReAct: Synergizing Reasoning and Acting in Language Models.**

    *Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao* [[pdf](https://arxiv.org/abs/2210.03629)] 2022.10

25. **Ask Me Anything: A simple strategy for prompting language models.**

    *Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, Christopher Ré* [[pdf](https://arxiv.org/abs/2210.02441)], [[code](https://github.com/HazyResearch/ama_prompting)] 2022.10

26. **Language Models of Code are Few-Shot Commonsense Learners.**

    *Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig* [[pdf](https://arxiv.org/abs/2210.07128)], [[code](https://github.com/madaan/cocogen)] 2022.10
    
27. **Large Language Models Can Self-Improve.**

    *Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han* [[pdf](https://arxiv.org/abs/2210.11610)] 2022.10
    
28. **Large Language Models are few(1)-shot Table Reasoners.**

    *Wenhu Chen* [[pdf](https://arxiv.org/abs/2210.06710)] 2022.10

39. **PAL: Program-aided Language Models.**

    *Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig* [[pdf](https://arxiv.org/abs/2211.10435)] 2022.11
    
30. **Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks.**

    *Wenhu Chen, Xueguang Ma, Xinyi Wang, William W. Cohen* [[pdf](https://wenhuchen.github.io/images/Program_of_Thoughts.pdf)] 2022.11
    
31. **Reasoning with Language Model Prompting: A Survey.**

    *Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen* [[pdf](https://arxiv.org/abs/2212.09597)] 2022.12
    
32. **Large Language Models are reasoners with Self-Verification.**

    *Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, Jun Zhao* [[pdf](https://arxiv.org/abs/2212.09561)] [[code](https://github.com/WENGSYX/Self-Verification)] 2022.12

33. **Language models are greedy reasoners: A systematic formal analysis of chain-of-thought** *ICLR2023*

    *Abulhair Saparov, He He* [[pdf](https://arxiv.org/abs/2210.01240)] [[code](https://github.com/asaparov/prontoqa)] 2022.10

34. **Specializing Smaller Language Models towards Multi-Step Reasoning**

    *Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar Khot* [[pdf](https://arxiv.org/abs/2301.12726)] 2023.01

35. **Is ChatGPT A Good Translator? A Preliminary Study**

    *Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, Zhaopeng Tu* [[pdf](https://arxiv.org/abs/2301.12726)] 2023.01

36. **Benchmarking Large Language Models for News Summarization**

    *Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, Tatsunori B. Hashimoto* [[pdf](https://export.arxiv.org/pdf/2301.13848)] 2023.01

37. **Large Language Models Can Be Easily Distracted by Irrelevant Context**
    *Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, Denny Zhou* [[pdf](https://arxiv.org/abs/2302.00093)] 2023.02

38. **Synthetic Prompting: Generating Chain-of-Thought Demonstrations for  Large Language Models**
    *Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen* [[pdf](https://arxiv.org/abs/2302.00618)] 2023.02

39. **Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning**

    *Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li* [[pdf](https://arxiv.org/abs/2301.13808)] 2023.02

40. **Multimodal Chain-of-Thought Reasoning in Language Models**

    *Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola* [[pdf](https://arxiv.org/abs/2302.00923)] [[code](https://github.com/amazon-science/mm-cot)] 2023.02

41. **Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers**

    *Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, Furu Wei* [[pdf](https://arxiv.org/pdf/2212.10559.pdf)] [[code](https://github.com/microsoft/LMOps)] 2022.12

42. **MetaPrompting: Learning to Learn Better Prompts**

    *Yutai Hou, Hongyuan Dong, Xinghao Wang, Bohan Li, Wanxiang Che,* [[pdf](https://arxiv.org/pdf/2209.11486.pdf)] [[code](https://github.com/Dousia/MetaPrompting)] 2023.2

43. **Languages are Rewards: Hindsight Finetuning using Human Feedback**

    *Hao Liu, Carmelo Sferrazza, Pieter Abbeel* [[pdf](https://arxiv.org/pdf/2302.02676.pdf)] 2023.2

44. **Rethinking with Retrieval: Faithful Large Language Model Inference**

    *Hangfeng He, Hongming Zhang, Dan Roth* [[pdf](https://arxiv.org/pdf/2301.00303.pdf)] [[code]( https://github.com/HornHehhf/RR)] 2022.12

45. **A Survey on In-context Learning**
    *Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui* [[pdf](https://export.arxiv.org/pdf/2301.00234)] 2023.1

46. **Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters.**

    *Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun* [[pdf](https://arxiv.org/abs/2212.10001)] [[code](https://github.com/sunlab-osu/Understanding-CoT)] 2022.12

47. **Large Language Models Are Reasoning Teachers.**

    *Namgyu Ho, Laura Schmid, Se-Young Yun* [[pdf](https://arxiv.org/abs/2212.10071)] 2022.12

48. **Translating Natural Language to Planning Goals with Large-Language  Models.**
    *Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, Harold Soh* [[pdf](https://arxiv.org/pdf/2302.05128.pdf)] 2023.2 

49. **The Wisdom of Hindsight Makes Language Models Better Instruction Followers**
    *Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, Joseph E. Gonzalez* [[pdf](https://arxiv.org/pdf/2302.05206.pdf)] 2023.2 

50. **Reliable Natural Language Understanding with Large Language Models and Answer Set Programming**
    *Abhiramon Rajasekharan, Yankai Zeng, Parth Padalkar, Gopal Gupta* [[pdf](https://arxiv.org/pdf/2302.03780.pdf)] 2023.2 
## Code LLMs
1. **Prompt Engineering for Solving CS1 Problems.**

    *Denny, Paul and Kumar, Viraj and Giacaman, Nasser* [[pdf](https://arxiv.org/abs/2210.15157)] 2022.10

2. **Language Models of Code are Few-Shot Commonsense Learners**

    *Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig* [[pdf](https://arxiv.org/abs/2210.07128)]

3. **Parsel: A (De-)compositional Framework for Algorithmic Reasoning with Language Models**

    *Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D. Goodman, Nick Haber* [[pdf](https://export.arxiv.org/pdf/2301.08745)] [[code](https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator)] 2023.1


## Evaluation of LLMs
1. **Holistic Evaluation of Language Models**

    *Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda* [[pdf](https://arxiv.org/abs/2211.09110)] [[code](https://github.com/stanford-crfm/helm)] 2022.11

2. **Evaluating Human-Language Model Interaction**

    *Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, Rose E. Wang, Minae Kwon, Joon Sung Park, Hancheng Cao, Tony Lee, Rishi Bommasani, Michael Bernstein, Percy Liang* [[pdf](https://arxiv.org/abs/2212.09746)] [[code](https://github.com/minggg/halie)] 2022.12

3. **Theory of Mind May Have Spontaneously Emerged in Large Language Models**
    *Michal Kosinski* [[pdf](https://arxiv.org/ftp/arxiv/papers/2302/2302.02083.pdf)] 2023.2

4. **A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity**
   *Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, Pascale Fung* [[pdf](https://arxiv.org/pdf/2302.04023.pdf)] 2023.2